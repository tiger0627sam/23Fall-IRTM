{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa4aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeb7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# # 資料夾路徑\n",
    "# folder_path = r'./data'\n",
    "\n",
    "# # 存儲文檔內容的列表\n",
    "# document_contents = []\n",
    "\n",
    "# # 列出資料夾中的所有文件\n",
    "# filenames = os.listdir(folder_path)\n",
    "\n",
    "# # 按照數字的順序排序文件名\n",
    "# def custom_sort(filename):\n",
    "\n",
    "#     match = re.match(r'(\\d+)', filename)\n",
    "#     if match:\n",
    "#         return int(match.group())\n",
    "#     return filename\n",
    "\n",
    "# # 排序\n",
    "# sorted_filenames = sorted(filenames, key=custom_sort)\n",
    "\n",
    "# for filename in sorted_filenames:\n",
    "#     # 文件路徑\n",
    "#     file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "#     if os.path.isfile(file_path):\n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             document_contents.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ccc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11', '19', '29', '113', '115', '169', '278', '301', '316', '317', '321', '324', '325', '338', '341'], ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16'], ['813', '817', '818', '819', '820', '821', '822', '824', '825', '826', '828', '829', '830', '832', '833'], ['635', '680', '683', '702', '704', '705', '706', '708', '709', '719', '720', '722', '723', '724', '726'], ['646', '751', '781', '794', '798', '799', '801', '812', '815', '823', '831', '839', '840', '841', '842'], ['995', '998', '999', '1003', '1005', '1006', '1007', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1019'], ['700', '730', '731', '732', '733', '735', '740', '744', '752', '754', '755', '756', '757', '759', '760'], ['262', '296', '304', '308', '337', '397', '401', '443', '445', '450', '466', '480', '513', '533', '534'], ['130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '145'], ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309', '315', '320', '326', '327', '328'], ['240', '241', '243', '244', '245', '248', '250', '254', '255', '256', '258', '260', '275', '279', '295'], ['535', '542', '571', '573', '574', '575', '576', '578', '581', '582', '583', '584', '585', '586', '588'], ['485', '520', '523', '526', '527', '529', '530', '531', '532', '536', '537', '538', '539', '540', '541']]\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "\n",
    "# 開啟檔案\n",
    "with open('training.txt', 'r') as file:\n",
    "    # 逐行讀取檔案內容\n",
    "    for line in file:\n",
    "        # 切割每一行的數字，去除第一個元素\n",
    "        category_data = line.strip().split()[1:]\n",
    "        \n",
    "        # 將字串串列加入categories List中\n",
    "        categories.append(category_data)\n",
    "\n",
    "# 顯示結果\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4895f676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['11', '19', '29', '113', '115', '169', '278', '301', '316',\n",
       "        '317', '321', '324', '325', '338', '341'],\n",
       "       ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13',\n",
       "        '14', '15', '16'],\n",
       "       ['813', '817', '818', '819', '820', '821', '822', '824', '825',\n",
       "        '826', '828', '829', '830', '832', '833'],\n",
       "       ['635', '680', '683', '702', '704', '705', '706', '708', '709',\n",
       "        '719', '720', '722', '723', '724', '726'],\n",
       "       ['646', '751', '781', '794', '798', '799', '801', '812', '815',\n",
       "        '823', '831', '839', '840', '841', '842'],\n",
       "       ['995', '998', '999', '1003', '1005', '1006', '1007', '1009',\n",
       "        '1011', '1012', '1013', '1014', '1015', '1016', '1019'],\n",
       "       ['700', '730', '731', '732', '733', '735', '740', '744', '752',\n",
       "        '754', '755', '756', '757', '759', '760'],\n",
       "       ['262', '296', '304', '308', '337', '397', '401', '443', '445',\n",
       "        '450', '466', '480', '513', '533', '534'],\n",
       "       ['130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
       "        '139', '140', '141', '142', '143', '145'],\n",
       "       ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309',\n",
       "        '315', '320', '326', '327', '328'],\n",
       "       ['240', '241', '243', '244', '245', '248', '250', '254', '255',\n",
       "        '256', '258', '260', '275', '279', '295'],\n",
       "       ['535', '542', '571', '573', '574', '575', '576', '578', '581',\n",
       "        '582', '583', '584', '585', '586', '588'],\n",
       "       ['485', '520', '523', '526', '527', '529', '530', '531', '532',\n",
       "        '536', '537', '538', '539', '540', '541']], dtype='<U4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#紀錄各類別訓練文件ID\n",
    "class_docid = np.array(categories)\n",
    "class_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc2f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有訓練文件ID\n",
    "tmp = set()\n",
    "for i in range(class_docid.shape[0]):\n",
    "    for j in range(class_docid[i].shape[0]):\n",
    "        tmp.add(class_docid[i][j])\n",
    "        \n",
    "#所有訓練文件ID      \n",
    "train_docid = list(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3256d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # empty List 用以儲存Tokens\n",
    "    tokens = []\n",
    "    \n",
    "    # empty String 用以儲存單字\n",
    "    current_token = \"\"\n",
    "    \n",
    "    separators = [' ', '.', ',', '!', '?', ';', ':', '_', '\\\"', '(', ')', '\\'', '`','@','$','%','&','*','{','/','-','#']\n",
    "    \n",
    "    # 追蹤每個字母\n",
    "    for char in text:\n",
    "        # 如果字母是空格或標點符號，並且current string is not empty，則將其添加到tokens List\n",
    "        if char.isspace() or char.isdigit()  or char in separators:\n",
    "            if current_token:\n",
    "                # 添加到 tokens 列表之前检查长度\n",
    "                if len(current_token) > 2:\n",
    "                    tokens.append(current_token)\n",
    "                current_token = \"\"\n",
    "        else:\n",
    "            # 如果字母不是空格或標點符號，則將其添加到current token\n",
    "            current_token += char\n",
    "    \n",
    "    # 將最後一個word添加到tokens列表中\n",
    "    if current_token and len(current_token) > 2 :\n",
    "        tokens.append(current_token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d1f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "# 讀取stopwords\n",
    "stopwords_file = open(\"NLTK's list of english stopwords.txt\", \"r\")\n",
    "stopwords = stopwords_file.read()\n",
    "stopwords_list = stopwords.splitlines()\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac2852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_path):\n",
    "    \n",
    "    # 存儲文檔內容的列表\n",
    "    document_contents = []\n",
    "\n",
    "    # 列出資料夾中的所有文件\n",
    "    filenames = os.listdir(data_path)\n",
    "\n",
    "    # 按照數字的順序排序文件名\n",
    "    def custom_sort(filename):\n",
    "\n",
    "        match = re.match(r'(\\d+)', filename)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return filename\n",
    "\n",
    "    # 排序\n",
    "    sorted_filenames = sorted(filenames, key=custom_sort)\n",
    "\n",
    "    for filename in sorted_filenames:\n",
    "        # 文件路徑\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                document_contents.append(file.read())\n",
    "    \n",
    "    processed_texts = []\n",
    "    \n",
    "    for document_content in text:\n",
    "        result = []\n",
    "        # lower casting\n",
    "        document_content = document_content.lower()\n",
    "        # Tokenized\n",
    "        tokenized_content = tokenize_text(document_content)\n",
    "        # Stopwords removal\n",
    "        filtered_tokens = [token for token in tokenized_content if token not in stopwords_list]\n",
    "        # Stemming\n",
    "        ps=PorterStemmer()\n",
    "        for t in filtered_tokens:  \n",
    "            result.append(ps.stem(t))\n",
    "        # 存入List\n",
    "        processed_texts.append(result)\n",
    "        \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281a22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(terms_list):\n",
    "\n",
    "    list_set = set(terms_list)\n",
    "    unique_list = (list(list_set))\n",
    " \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575c54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVoc(path):\n",
    "    #對每個 \"訓練 \"文件做前處理：以 array 儲存 list of terms以及相對應的 docID\n",
    "    dir_path = path\n",
    "    listdir = os.listdir(dir_path)\n",
    "    term_docid = np.array([[0,0]])\n",
    "\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        #以training doc製作dictionary\n",
    "        if str(i) in train_docid:\n",
    "            data_path = dir_path + str(i) + '.txt'\n",
    "            term = preprocessing(data_path)\n",
    "            term = get_unique(term)\n",
    "            tmp = np.asarray([np.asarray(term), np.repeat(int(i), len(term))]).T\n",
    "            term_docid = np.concatenate((term_docid, tmp))\n",
    "\n",
    "    term_docid = term_docid[1:]\n",
    "    \n",
    "    df = pd.DataFrame(term_docid, columns = ['term', 'doc_id'])\n",
    "    df = df.sort_values(by = [\"term\", \"doc_id\"])\n",
    "    df = df.to_numpy()\n",
    "    \n",
    "    index = 0\n",
    "    count = 1\n",
    "    term_df = np.array([[0,0]])\n",
    "\n",
    "    while(index < len(df)): \n",
    "        if index+1 < len(df) and df[index][0] == df[index+1][0]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            term_df = np.concatenate((term_df, np.reshape(([df[index][0], count]), (-1,2))))\n",
    "            count = 1\n",
    "        index = index + 1\n",
    "\n",
    "    term_df = term_df[1:]\n",
    "    \n",
    "    return term_df[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa3607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountDocs(path):\n",
    "    return len(train_docid)\n",
    "\n",
    "\n",
    "def ConcatenateAllDocInClass(path, class_id):\n",
    "    data_path = path\n",
    "    listdir = os.listdir(data_path)\n",
    "    \n",
    "    text = []\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        if str(i) in class_docid[class_id-1]:\n",
    "            dir_path = data_path + str(i) + '.txt'\n",
    "            term = preprocessing(dir_path)\n",
    "            text.extend(term)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5922c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB(C, path):\n",
    "    path = path\n",
    "    V = extractVoc(path) #V原始字典array\n",
    "    N = CountDocs(path) \n",
    "\n",
    "#     V_ = featureSelect(V, C, path) \n",
    "    \n",
    "    prior = np.zeros(C.shape[0])\n",
    "    condprob = np.zeros((V.shape[0], C.shape[0]))\n",
    "    \n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        N_class_id = len(C[class_id-1])\n",
    "        prior[class_id-1] = N_class_id / N\n",
    "        text_class_id = ConcatenateAllDocInClass(path, class_id)\n",
    "        \n",
    "        count_term_sum = 0\n",
    "        for term in V:\n",
    "            count_term = text_class_id.count(term)\n",
    "            count_term_sum = count_term_sum + count_term\n",
    "            \n",
    "        for term_id, term in enumerate(V): #term_id starts from 0\n",
    "            count_term = text_class_id.count(term)\n",
    "            condprob[term_id][class_id-1] = (count_term + 1)/(count_term_sum + V.shape[0])\n",
    "    \n",
    "    return V, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a60aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractTokensFromDoc(V, d):\n",
    "    data_path = \"./data/\" + str(d) + \".txt\"\n",
    "    tokens = preprocessing(data_path)\n",
    "    \n",
    "    W = []\n",
    "    for token in tokens:\n",
    "        if token in V:\n",
    "            W.append(token)     \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c18aede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NB(C, V, prior, condprob, d): #d:doc_id\n",
    "    W = ExtractTokensFromDoc(V, d)\n",
    "    \n",
    "    score = np.zeros(C.shape[0])\n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        score[class_id-1] = np.log10(prior[class_id-1])\n",
    "        \n",
    "        for term in W:\n",
    "            term_id = np.where(V == term)[0][0] #term_id starts from 0\n",
    "            score[class_id-1] = score[class_id-1] + np.log10(condprob[term_id][class_id-1])\n",
    "            \n",
    "    return np.argmax(score)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab62483",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] 目錄名稱無效。: './data/1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m V, prior, condprob \u001b[38;5;241m=\u001b[39m train_NB(C, path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#testing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(listdir)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtrain_NB\u001b[1;34m(C, path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_NB\u001b[39m(C, path):\n\u001b[0;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m----> 3\u001b[0m     V \u001b[38;5;241m=\u001b[39m extractVoc(path) \u001b[38;5;66;03m#V原始字典array\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     N \u001b[38;5;241m=\u001b[39m CountDocs(path) \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     V_ = featureSelect(V, C, path) \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mextractVoc\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;129;01min\u001b[39;00m train_docid:\n\u001b[0;32m     10\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m dir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     term \u001b[38;5;241m=\u001b[39m preprocessing(data_path)\n\u001b[0;32m     12\u001b[0m     term \u001b[38;5;241m=\u001b[39m get_unique(term)\n\u001b[0;32m     13\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([np\u001b[38;5;241m.\u001b[39masarray(term), np\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mint\u001b[39m(i), \u001b[38;5;28mlen\u001b[39m(term))])\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m document_contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 列出資料夾中的所有文件\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m filenames \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(data_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 按照數字的順序排序文件名\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_sort\u001b[39m(filename):\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] 目錄名稱無效。: './data/1.txt'"
     ]
    }
   ],
   "source": [
    "C = class_docid\n",
    "path = r'./data/'\n",
    "listdir = os.listdir(path)\n",
    "result = []\n",
    "#training\n",
    "V, prior, condprob = train_NB(C, path)\n",
    "#testing\n",
    "for i in range(1, len(listdir)+1):\n",
    "    if str(i) not in train_docid:\n",
    "        data_path = path + str(i) + \".txt\"\n",
    "        class_id = test_NB(C, V, prior, condprob, i)\n",
    "        result.append([i, class_id])\n",
    "#save as result.csv\n",
    "df = pd.DataFrame(data = result, columns = [\"Id\", \"Value\"])\n",
    "df.to_csv(\"result.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04956752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Empty list 用以儲存每個單字的index,term,frequency\n",
    "# term_info_list = []\n",
    "\n",
    "# # Empty dict for storing term_document_frequency\n",
    "# term_document_frequency = {}\n",
    "\n",
    "# # Loop start\n",
    "# for words in processed_texts:\n",
    "#     # 避免重複計算\n",
    "#     unique_words = set(words)\n",
    "    \n",
    "#     # 更新Frequency\n",
    "#     for word in unique_words:\n",
    "#         if word in term_document_frequency:\n",
    "#             term_document_frequency[word] += 1\n",
    "#         else:\n",
    "#             term_document_frequency[word] = 1\n",
    "\n",
    "# # 按照字母順序排列字典\n",
    "# sorted_terms = sorted(term_document_frequency.items(), key=lambda x: x[0])\n",
    "\n",
    "# # 紀錄 index並存入 term_info_list 中\n",
    "# for index, (term, frequency) in enumerate(sorted_terms, start=1):\n",
    "#     term_info = {'index': index, 'term': term, 'frequency': frequency}\n",
    "#     term_info_list.append(term_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Empty List for storing each document\n",
    "# document_term_frequencies = []\n",
    "\n",
    "# # Loop start\n",
    "# for words in processed_texts:\n",
    "#     # Empty dict for storing document_term_frequency\n",
    "#     document_term_frequency = {}\n",
    "    \n",
    "#     # 更新frequency\n",
    "#     for word in words:\n",
    "#         if word in document_term_frequency:\n",
    "#             document_term_frequency[word] += 1\n",
    "#         else:\n",
    "#             document_term_frequency[word] = 1\n",
    "    \n",
    "#     # 存入個文件的 List\n",
    "#     document_term_frequencies.append(document_term_frequency)\n",
    "\n",
    "# # 計算文件總數\n",
    "# total_documents = len(processed_texts)\n",
    "\n",
    "# tfidf_vectors = []\n",
    "\n",
    "# for document_term_frequency in document_term_frequencies:\n",
    "#     tfidf_vector = []\n",
    "            \n",
    "#     for word, term_frequency in document_term_frequency.items():\n",
    "#         # 計算 TF\n",
    "#         tf = term_frequency / sum(document_term_frequency.values())\n",
    "        \n",
    "#         # 計算 IDF\n",
    "#         idf = math.log10(total_documents / term_document_frequency[word])\n",
    "        \n",
    "#         # 計算 TF-IDF\n",
    "#         tfidf = tf * idf\n",
    "        \n",
    "#         # 獲取 term_index\n",
    "#         term_index = next((item['index'] for item in term_info_list if item['term'] == word), None)\n",
    "        \n",
    "#         # Add term_index and TF-IDF 值到向量\n",
    "#         tfidf_info = {'index': term_index, 'term': word, 'tf-idf': tfidf, 'tf' : tf, 'idf' : idf}\n",
    "#         tfidf_vector.append(tfidf_info)\n",
    "    \n",
    "#     tfidf_vectors.append(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_vectors[597])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 初始化13個字典\n",
    "# category_dicts = [{} for _ in range(13)]\n",
    "\n",
    "# # 遍歷每一個類別\n",
    "# for i, category_indices in enumerate(categories):\n",
    "#     # 將類別中的每一個文章編號轉換為整數\n",
    "#     category_indices = list(map(int, category_indices))\n",
    "\n",
    "#     # 遍歷類別中的每一個文章編號\n",
    "#     for idx in category_indices:\n",
    "#         # 取得文章編號對應的文本\n",
    "#         text = processed_texts[idx]\n",
    "\n",
    "#         # 將文本中的每個字加入到字典中\n",
    "#         for word in text:\n",
    "#             if word in category_dicts[i]:\n",
    "#                 category_dicts[i][word] += 1\n",
    "#             else:\n",
    "#                 category_dicts[i][word] = 1\n",
    "\n",
    "# # 顯示結果\n",
    "# for i, category_dict in enumerate(category_dicts, start=1):\n",
    "#     print(f\"Category {i} Dictionary:\")\n",
    "#     print(category_dict)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 將字詞的統計表轉換為 DataFrame\n",
    "# df = pd.DataFrame(category_dicts)\n",
    "\n",
    "# # 將 DataFrame 轉換為 NumPy array 以供模型使用\n",
    "# X_train = df.values\n",
    "# y_train = df.columns\n",
    "\n",
    "# # 處理測試集的字詞統計\n",
    "# test_word_counts_list = []\n",
    "\n",
    "# for i, words in enumerate(processed_texts):\n",
    "#     test_word_counts = {}\n",
    "#     for word in words:\n",
    "#         if word in test_word_counts:\n",
    "#             test_word_counts[word] += 1\n",
    "#         else:\n",
    "#             test_word_counts[word] = 1\n",
    "#     test_word_counts_list.append(test_word_counts)\n",
    "\n",
    "# # 將測試集字詞的統計表轉換為 DataFrame\n",
    "# test_df = pd.DataFrame(test_word_counts_list).fillna(0)\n",
    "\n",
    "# # 將 DataFrame 轉換為 NumPy array 以供模型使用\n",
    "# X_test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
