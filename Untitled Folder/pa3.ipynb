{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa4aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeb7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ccc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11', '19', '29', '113', '115', '169', '278', '301', '316', '317', '321', '324', '325', '338', '341'], ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16'], ['813', '817', '818', '819', '820', '821', '822', '824', '825', '826', '828', '829', '830', '832', '833'], ['635', '680', '683', '702', '704', '705', '706', '708', '709', '719', '720', '722', '723', '724', '726'], ['646', '751', '781', '794', '798', '799', '801', '812', '815', '823', '831', '839', '840', '841', '842'], ['995', '998', '999', '1003', '1005', '1006', '1007', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1019'], ['700', '730', '731', '732', '733', '735', '740', '744', '752', '754', '755', '756', '757', '759', '760'], ['262', '296', '304', '308', '337', '397', '401', '443', '445', '450', '466', '480', '513', '533', '534'], ['130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '145'], ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309', '315', '320', '326', '327', '328'], ['240', '241', '243', '244', '245', '248', '250', '254', '255', '256', '258', '260', '275', '279', '295'], ['535', '542', '571', '573', '574', '575', '576', '578', '581', '582', '583', '584', '585', '586', '588'], ['485', '520', '523', '526', '527', '529', '530', '531', '532', '536', '537', '538', '539', '540', '541']]\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "\n",
    "# 開啟檔案\n",
    "with open('training.txt', 'r') as file:\n",
    "    # 逐行讀取檔案內容\n",
    "    for line in file:\n",
    "        # 切割每一行的數字，去除第一個元素\n",
    "        category_data = line.strip().split()[1:]\n",
    "        \n",
    "        # 將字串串列加入categories List中\n",
    "        categories.append(category_data)\n",
    "\n",
    "# 顯示結果\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4895f676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['11', '19', '29', '113', '115', '169', '278', '301', '316',\n",
       "        '317', '321', '324', '325', '338', '341'],\n",
       "       ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13',\n",
       "        '14', '15', '16'],\n",
       "       ['813', '817', '818', '819', '820', '821', '822', '824', '825',\n",
       "        '826', '828', '829', '830', '832', '833'],\n",
       "       ['635', '680', '683', '702', '704', '705', '706', '708', '709',\n",
       "        '719', '720', '722', '723', '724', '726'],\n",
       "       ['646', '751', '781', '794', '798', '799', '801', '812', '815',\n",
       "        '823', '831', '839', '840', '841', '842'],\n",
       "       ['995', '998', '999', '1003', '1005', '1006', '1007', '1009',\n",
       "        '1011', '1012', '1013', '1014', '1015', '1016', '1019'],\n",
       "       ['700', '730', '731', '732', '733', '735', '740', '744', '752',\n",
       "        '754', '755', '756', '757', '759', '760'],\n",
       "       ['262', '296', '304', '308', '337', '397', '401', '443', '445',\n",
       "        '450', '466', '480', '513', '533', '534'],\n",
       "       ['130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
       "        '139', '140', '141', '142', '143', '145'],\n",
       "       ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309',\n",
       "        '315', '320', '326', '327', '328'],\n",
       "       ['240', '241', '243', '244', '245', '248', '250', '254', '255',\n",
       "        '256', '258', '260', '275', '279', '295'],\n",
       "       ['535', '542', '571', '573', '574', '575', '576', '578', '581',\n",
       "        '582', '583', '584', '585', '586', '588'],\n",
       "       ['485', '520', '523', '526', '527', '529', '530', '531', '532',\n",
       "        '536', '537', '538', '539', '540', '541']], dtype='<U4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#紀錄各類別訓練文件ID\n",
    "class_docid = np.array(categories)\n",
    "class_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc2f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有訓練文件ID\n",
    "tmp = set()\n",
    "for i in range(class_docid.shape[0]):\n",
    "    for j in range(class_docid[i].shape[0]):\n",
    "        tmp.add(class_docid[i][j])\n",
    "        \n",
    "#所有訓練文件ID      \n",
    "train_docid = list(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3256d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # empty List 用以儲存Tokens\n",
    "    tokens = []\n",
    "    \n",
    "    # empty String 用以儲存單字\n",
    "    current_token = \"\"\n",
    "    \n",
    "    separators = [' ', '.', ',', '!', '?', ';', ':', '_', '\\\"', '(', ')', '\\'', '`','@','$','%','&','*','{','/','-','#']\n",
    "    \n",
    "    # 追蹤每個字母\n",
    "    for char in text:\n",
    "        # 如果字母是空格或標點符號，並且current string is not empty，則將其添加到tokens List\n",
    "        if char.isspace() or char.isdigit()  or char in separators:\n",
    "            if current_token:\n",
    "                # 添加到 tokens 列表之前检查长度\n",
    "                if len(current_token) > 2:\n",
    "                    tokens.append(current_token)\n",
    "                current_token = \"\"\n",
    "        else:\n",
    "            # 如果字母不是空格或標點符號，則將其添加到current token\n",
    "            current_token += char\n",
    "    \n",
    "    # 將最後一個word添加到tokens列表中\n",
    "    if current_token and len(current_token) > 2 :\n",
    "        tokens.append(current_token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d1f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "# 讀取stopwords\n",
    "stopwords_file = open(\"NLTK's list of english stopwords.txt\", \"r\")\n",
    "stopwords = stopwords_file.read()\n",
    "stopwords_list = stopwords.splitlines()\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac2852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_path):\n",
    "    \n",
    "    path = data_path\n",
    "    f = open(path, 'r')\n",
    "    document_content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    processed_texts = []\n",
    "\n",
    "    # lower casting\n",
    "    document_content = document_content.lower()\n",
    "    # Tokenized\n",
    "    tokenized_content = tokenize_text(document_content)\n",
    "    # Stopwords removal\n",
    "    filtered_tokens = [token for token in tokenized_content if token not in stopwords_list]\n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    for t in filtered_tokens:  \n",
    "        processed_texts.append(ps.stem(t))\n",
    "        \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc00497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing2(data_path):\n",
    "    path = data_path\n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    #Removing punctuation\n",
    "    punc = '''!()-[]{}|;:'\"`\\,<=>./?@#$%^&*+-_~'''\n",
    "    for ele in text:\n",
    "        if ele in punc:\n",
    "            text = text.replace(ele, \"\")    \n",
    "    #Tokenization\n",
    "    text = text.split()\n",
    "    #Lowercasing\n",
    "    lower_text = []\n",
    "    for token in text:\n",
    "        lower_text.append(token.lower())\n",
    "    #Stemming with Porter's algo.\n",
    "    #pip install --user -U nltk\n",
    "    ps = PorterStemmer()\n",
    "    stem_text = []\n",
    "    for token in lower_text:\n",
    "        stem_text.append(ps.stem(token)) \n",
    "    #Remove digit\n",
    "    stem_text = [item for item in stem_text if not item.isdigit()]\n",
    "    #Stopwords removal\n",
    "    removesw_text = []\n",
    "    for token in stem_text:\n",
    "        token = ''.join((x for x in token if not x.isdigit()))\n",
    "        if token not in stopwords_list:\n",
    "            removesw_text.append(token)\n",
    "            \n",
    "    return removesw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281a22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(terms_list):\n",
    "\n",
    "    list_set = set(terms_list)\n",
    "    unique_list = (list(list_set))\n",
    " \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575c54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVoc(path):\n",
    "    #對每個 \"訓練 \"文件做前處理：以 array 儲存 list of terms以及相對應的 docID\n",
    "    dir_path = path\n",
    "    listdir = os.listdir(dir_path)\n",
    "    term_docid = np.array([[0,0]])\n",
    "\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        #以training doc製作dictionary\n",
    "        if str(i) in train_docid:\n",
    "            data_path = dir_path + str(i) + '.txt'\n",
    "            term = preprocessing(data_path)\n",
    "            term = get_unique(term)\n",
    "            tmp = np.asarray([np.asarray(term), np.repeat(int(i), len(term))]).T\n",
    "            term_docid = np.concatenate((term_docid, tmp))\n",
    "\n",
    "    term_docid = term_docid[1:]\n",
    "    \n",
    "    df = pd.DataFrame(term_docid, columns = ['term', 'doc_id'])\n",
    "    df = df.sort_values(by = [\"term\", \"doc_id\"])\n",
    "    df = df.to_numpy()\n",
    "    \n",
    "    index = 0\n",
    "    count = 1\n",
    "    term_df = np.array([[0,0]])\n",
    "\n",
    "    while(index < len(df)): \n",
    "        if index+1 < len(df) and df[index][0] == df[index+1][0]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            term_df = np.concatenate((term_df, np.reshape(([df[index][0], count]), (-1,2))))\n",
    "            count = 1\n",
    "        index = index + 1\n",
    "\n",
    "    term_df = term_df[1:]\n",
    "    \n",
    "    return term_df[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa3607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountDocs(path):\n",
    "    return len(train_docid)\n",
    "\n",
    "\n",
    "def ConcatenateAllDocInClass(path, class_id):\n",
    "    data_path = path\n",
    "    listdir = os.listdir(data_path)\n",
    "    \n",
    "    text = []\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        if str(i) in class_docid[class_id-1]:\n",
    "            dir_path = data_path + str(i) + '.txt'\n",
    "            term = preprocessing(dir_path)\n",
    "            text.extend(term)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5922c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB(C, path):\n",
    "    path = path\n",
    "    V = extractVoc(path) #V原始字典array\n",
    "    N = CountDocs(path) \n",
    "\n",
    "#     V_ = featureSelect(V, C, path) \n",
    "    \n",
    "    prior = np.zeros(C.shape[0])\n",
    "    condprob = np.zeros((V.shape[0], C.shape[0]))\n",
    "    \n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        N_class_id = len(C[class_id-1])\n",
    "        prior[class_id-1] = N_class_id / N\n",
    "        text_class_id = ConcatenateAllDocInClass(path, class_id)\n",
    "        \n",
    "        count_term_sum = 0\n",
    "        for term in V:\n",
    "            count_term = text_class_id.count(term)\n",
    "            count_term_sum = count_term_sum + count_term\n",
    "            \n",
    "        for term_id, term in enumerate(V): #term_id starts from 0\n",
    "            count_term = text_class_id.count(term)\n",
    "            condprob[term_id][class_id-1] = (count_term + 1)/(count_term_sum + V.shape[0])\n",
    "    \n",
    "    return V, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a60aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractTokensFromDoc(V, d):\n",
    "    data_path = \"./data/\" + str(d) + \".txt\"\n",
    "    tokens = preprocessing(data_path)\n",
    "    \n",
    "    W = []\n",
    "    for token in tokens:\n",
    "        if token in V:\n",
    "            W.append(token)     \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c18aede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NB(C, V, prior, condprob, d): #d:doc_id\n",
    "    W = ExtractTokensFromDoc(V, d)\n",
    "    \n",
    "    score = np.zeros(C.shape[0])\n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        score[class_id-1] = np.log10(prior[class_id-1])\n",
    "        \n",
    "        for term in W:\n",
    "            term_id = np.where(V == term)[0][0] #term_id starts from 0\n",
    "            score[class_id-1] = score[class_id-1] + np.log10(condprob[term_id][class_id-1])\n",
    "            \n",
    "    return np.argmax(score)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab62483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "C = class_docid\n",
    "path = './data/'\n",
    "listdir = os.listdir(path)\n",
    "result = []\n",
    "#training\n",
    "V, prior, condprob = train_NB(C, path)\n",
    "#testing\n",
    "for i in range(1, len(listdir)+1):\n",
    "    if str(i) not in train_docid:\n",
    "        data_path = path + str(i) + \".txt\"\n",
    "        class_id = test_NB(C, V, prior, condprob, i)\n",
    "        result.append([i, class_id])\n",
    "#save as result.csv\n",
    "df = pd.DataFrame(data = result, columns = [\"Id\", \"Value\"])\n",
    "df.to_csv(\"result.csv\", index=False)\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
