{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa4aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeb7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ccc9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11', '19', '29', '113', '115', '169', '278', '301', '316', '317', '321', '324', '325', '338', '341'], ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16'], ['813', '817', '818', '819', '820', '821', '822', '824', '825', '826', '828', '829', '830', '832', '833'], ['635', '680', '683', '702', '704', '705', '706', '708', '709', '719', '720', '722', '723', '724', '726'], ['646', '751', '781', '794', '798', '799', '801', '812', '815', '823', '831', '839', '840', '841', '842'], ['995', '998', '999', '1003', '1005', '1006', '1007', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1019'], ['700', '730', '731', '732', '733', '735', '740', '744', '752', '754', '755', '756', '757', '759', '760'], ['262', '296', '304', '308', '337', '397', '401', '443', '445', '450', '466', '480', '513', '533', '534'], ['130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '145'], ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309', '315', '320', '326', '327', '328'], ['240', '241', '243', '244', '245', '248', '250', '254', '255', '256', '258', '260', '275', '279', '295'], ['535', '542', '571', '573', '574', '575', '576', '578', '581', '582', '583', '584', '585', '586', '588'], ['485', '520', '523', '526', '527', '529', '530', '531', '532', '536', '537', '538', '539', '540', '541']]\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "\n",
    "# 開啟檔案\n",
    "with open('training.txt', 'r') as file:\n",
    "    # 逐行讀取檔案內容\n",
    "    for line in file:\n",
    "        # 切割每一行的數字，去除第一個元素\n",
    "        category_data = line.strip().split()[1:]\n",
    "        \n",
    "        # 將字串串列加入categories List中\n",
    "        categories.append(category_data)\n",
    "\n",
    "# 顯示結果\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4895f676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['11', '19', '29', '113', '115', '169', '278', '301', '316',\n",
       "        '317', '321', '324', '325', '338', '341'],\n",
       "       ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13',\n",
       "        '14', '15', '16'],\n",
       "       ['813', '817', '818', '819', '820', '821', '822', '824', '825',\n",
       "        '826', '828', '829', '830', '832', '833'],\n",
       "       ['635', '680', '683', '702', '704', '705', '706', '708', '709',\n",
       "        '719', '720', '722', '723', '724', '726'],\n",
       "       ['646', '751', '781', '794', '798', '799', '801', '812', '815',\n",
       "        '823', '831', '839', '840', '841', '842'],\n",
       "       ['995', '998', '999', '1003', '1005', '1006', '1007', '1009',\n",
       "        '1011', '1012', '1013', '1014', '1015', '1016', '1019'],\n",
       "       ['700', '730', '731', '732', '733', '735', '740', '744', '752',\n",
       "        '754', '755', '756', '757', '759', '760'],\n",
       "       ['262', '296', '304', '308', '337', '397', '401', '443', '445',\n",
       "        '450', '466', '480', '513', '533', '534'],\n",
       "       ['130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
       "        '139', '140', '141', '142', '143', '145'],\n",
       "       ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309',\n",
       "        '315', '320', '326', '327', '328'],\n",
       "       ['240', '241', '243', '244', '245', '248', '250', '254', '255',\n",
       "        '256', '258', '260', '275', '279', '295'],\n",
       "       ['535', '542', '571', '573', '574', '575', '576', '578', '581',\n",
       "        '582', '583', '584', '585', '586', '588'],\n",
       "       ['485', '520', '523', '526', '527', '529', '530', '531', '532',\n",
       "        '536', '537', '538', '539', '540', '541']], dtype='<U4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#紀錄各類別訓練文件ID\n",
    "class_docid = np.array(categories)\n",
    "class_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc2f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有訓練文件ID\n",
    "tmp = set()\n",
    "for i in range(class_docid.shape[0]):\n",
    "    for j in range(class_docid[i].shape[0]):\n",
    "        tmp.add(class_docid[i][j])\n",
    "        \n",
    "#所有訓練文件ID      \n",
    "train_docid = list(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3256d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # empty List 用以儲存Tokens\n",
    "    tokens = []\n",
    "    \n",
    "    # empty String 用以儲存單字\n",
    "    current_token = \"\"\n",
    "    \n",
    "    separators = [' ', '.', ',', '!', '?', ';', ':', '_', '\\\"', '(', ')', '\\'', '`','@','$','%','&','*','{','/','-','#']\n",
    "    \n",
    "    # 追蹤每個字母\n",
    "    for char in text:\n",
    "        # 如果字母是空格或標點符號，並且current string is not empty，則將其添加到tokens List\n",
    "        if char.isspace() or char.isdigit()  or char in separators:\n",
    "            if current_token:\n",
    "                # 添加到 tokens 列表之前检查长度\n",
    "                if len(current_token) > 2:\n",
    "                    tokens.append(current_token)\n",
    "                current_token = \"\"\n",
    "        else:\n",
    "            # 如果字母不是空格或標點符號，則將其添加到current token\n",
    "            current_token += char\n",
    "    \n",
    "    # 將最後一個word添加到tokens列表中\n",
    "    if current_token and len(current_token) > 2 :\n",
    "        tokens.append(current_token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d1f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "# 讀取stopwords\n",
    "stopwords_file = open(\"NLTK's list of english stopwords.txt\", \"r\")\n",
    "stopwords = stopwords_file.read()\n",
    "stopwords_list = stopwords.splitlines()\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac2852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_path):\n",
    "    \n",
    "    path = data_path\n",
    "    f = open(path, 'r')\n",
    "    document_content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    processed_texts = []\n",
    "\n",
    "    # lower casting\n",
    "    document_content = document_content.lower()\n",
    "    # Tokenized\n",
    "    tokenized_content = tokenize_text(document_content)\n",
    "    # Stopwords removal\n",
    "    filtered_tokens = [token for token in tokenized_content if token not in stopwords_list]\n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    for t in filtered_tokens:  \n",
    "        processed_texts.append(ps.stem(t))\n",
    "        \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc00497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing2(data_path):\n",
    "    path = data_path\n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    #Removing punctuation\n",
    "    punc = '''!()-[]{}|;:'\"`\\,<=>./?@#$%^&*+-_~'''\n",
    "    for ele in text:\n",
    "        if ele in punc:\n",
    "            text = text.replace(ele, \"\")    \n",
    "    #Tokenization\n",
    "    text = text.split()\n",
    "    #Lowercasing\n",
    "    lower_text = []\n",
    "    for token in text:\n",
    "        lower_text.append(token.lower())\n",
    "    #Stemming with Porter's algo.\n",
    "    #pip install --user -U nltk\n",
    "    ps = PorterStemmer()\n",
    "    stem_text = []\n",
    "    for token in lower_text:\n",
    "        stem_text.append(ps.stem(token)) \n",
    "    #Remove digit\n",
    "    stem_text = [item for item in stem_text if not item.isdigit()]\n",
    "    #Stopwords removal\n",
    "    removesw_text = []\n",
    "    for token in stem_text:\n",
    "        token = ''.join((x for x in token if not x.isdigit()))\n",
    "        if token not in stopwords_list:\n",
    "            removesw_text.append(token)\n",
    "            \n",
    "    return removesw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281a22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(terms_list):\n",
    "\n",
    "    list_set = set(terms_list)\n",
    "    unique_list = (list(list_set))\n",
    " \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575c54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVoc(path):\n",
    "    #對每個 \"訓練 \"文件做前處理：以 array 儲存 list of terms以及相對應的 docID\n",
    "    dir_path = path\n",
    "    listdir = os.listdir(dir_path)\n",
    "    term_docid = np.array([[0,0]])\n",
    "\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        #以training doc製作dictionary\n",
    "        if str(i) in train_docid:\n",
    "            data_path = dir_path + str(i) + '.txt'\n",
    "            term = preprocessing(data_path)\n",
    "            term = get_unique(term)\n",
    "            tmp = np.asarray([np.asarray(term), np.repeat(int(i), len(term))]).T\n",
    "            term_docid = np.concatenate((term_docid, tmp))\n",
    "\n",
    "    term_docid = term_docid[1:]\n",
    "    \n",
    "    df = pd.DataFrame(term_docid, columns = ['term', 'doc_id'])\n",
    "    df = df.sort_values(by = [\"term\", \"doc_id\"])\n",
    "    df = df.to_numpy()\n",
    "    \n",
    "    index = 0\n",
    "    count = 1\n",
    "    term_df = np.array([[0,0]])\n",
    "\n",
    "    while(index < len(df)): \n",
    "        if index+1 < len(df) and df[index][0] == df[index+1][0]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            term_df = np.concatenate((term_df, np.reshape(([df[index][0], count]), (-1,2))))\n",
    "            count = 1\n",
    "        index = index + 1\n",
    "\n",
    "    term_df = term_df[1:]\n",
    "    \n",
    "    return term_df[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa3607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountDocs(path):\n",
    "    return len(train_docid)\n",
    "\n",
    "\n",
    "def ConcatenateAllDocInClass(path, class_id):\n",
    "    data_path = path\n",
    "    listdir = os.listdir(data_path)\n",
    "    \n",
    "    text = []\n",
    "    for i in range(1, len(listdir)+1):\n",
    "        if str(i) in class_docid[class_id-1]:\n",
    "            dir_path = data_path + str(i) + '.txt'\n",
    "            term = preprocessing(dir_path)\n",
    "            text.extend(term)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5922c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB(C, path):\n",
    "    path = path\n",
    "    V = extractVoc(path) #V原始字典array\n",
    "    N = CountDocs(path)\n",
    "    \n",
    "\n",
    "    V_ = cal_square_test(V, C, path)\n",
    "    \n",
    "    prior = np.zeros(C.shape[0])\n",
    "    condprob = np.zeros((V_.shape[0], C.shape[0]))\n",
    "    \n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        N_class_id = len(C[class_id-1])\n",
    "        prior[class_id-1] = N_class_id / N\n",
    "        text_class_id = ConcatenateAllDocInClass(path, class_id)\n",
    "        \n",
    "        count_term_sum = 0\n",
    "        for term in V_:\n",
    "            count_term = text_class_id.count(term)\n",
    "            count_term_sum = count_term_sum + count_term\n",
    "            \n",
    "        for term_id, term in enumerate(V_): #term_id starts from 0\n",
    "            count_term = text_class_id.count(term)\n",
    "            condprob[term_id][class_id-1] = (count_term + 1)/(count_term_sum + V_.shape[0])\n",
    "    \n",
    "    return V, V_, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a60aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractTokensFromDoc(V, d):\n",
    "    data_path = \"./data/\" + str(d) + \".txt\"\n",
    "    tokens = preprocessing(data_path)\n",
    "    \n",
    "    W = []\n",
    "    for token in tokens:\n",
    "        if token in V:\n",
    "            W.append(token)     \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c18aede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NB(C, V, prior, condprob, d): #d:doc_id\n",
    "    W = ExtractTokensFromDoc(V, d)\n",
    "    \n",
    "    score = np.zeros(C.shape[0])\n",
    "    for class_id in range(1, C.shape[0]+1):\n",
    "        score[class_id-1] = np.log10(prior[class_id-1])\n",
    "        \n",
    "        for term in W:\n",
    "            term_id = np.where(V == term)[0][0] #term_id starts from 0\n",
    "            score[class_id-1] = score[class_id-1] + np.log10(condprob[term_id][class_id-1])\n",
    "            \n",
    "    return np.argmax(score)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2d02cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(V, C, path):\n",
    "    # V: 詞彙列表\n",
    "    # C: 分類列表\n",
    "    # term_doc_matrix: 詞彙-文檔矩陣\n",
    "    \n",
    "    doc_class_list = generate_doc_class_list(C)\n",
    "    term_doc_matrix = generate_term_doc_matrix(V, C, path)\n",
    "    \n",
    "    print(term_doc_matrix)\n",
    "    \n",
    "    # 初始化Χ²分數的字典\n",
    "    chi_square_scores = defaultdict(float)\n",
    "\n",
    "    # 計算所有文檔中所有詞彙的出現次數\n",
    "    total_terms = term_doc_matrix.sum()\n",
    "\n",
    "    # 對於每個詞彙，計算它在每個類別中的Χ²分數\n",
    "    for term_id, term in enumerate(V):\n",
    "        # 計算 term 在所有類別中的出現次數\n",
    "        term_count_total = term_doc_matrix[term_id, :].sum()\n",
    "\n",
    "        for class_id in C:\n",
    "            # 計算 class_id 中所有詞彙的出現次數\n",
    "            total_terms_in_class = term_doc_matrix[:, doc_class_list == class_id].sum()\n",
    "            # 計算 term 在 class_id 中的出現次數\n",
    "            term_count_in_class = term_doc_matrix[term_id, doc_class_list == class_id].sum()\n",
    "\n",
    "            # 計算期望頻率和觀察頻率\n",
    "            expected_frequency = total_terms_in_class * term_count_total / total_terms\n",
    "            observed_frequency = term_count_in_class\n",
    "\n",
    "            # Χ²分數計算\n",
    "            if expected_frequency > 0:\n",
    "                chi_square_score = ((observed_frequency - expected_frequency) ** 2) / expected_frequency\n",
    "                chi_square_scores[term] += chi_square_score\n",
    "\n",
    "    # 根據Χ²分數選擇前500個詞彙\n",
    "    selected_terms = sorted(chi_square_scores, key=chi_square_scores.get, reverse=True)[:500]\n",
    "\n",
    "    return selected_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438a45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_class_list(C):\n",
    "    # C: 分類列表，其中每個元素是一個包含特定類別文檔ID的列表\n",
    "    doc_class_list = {}\n",
    "\n",
    "    # 對每個類別和其對應的文檔ID進行迭代\n",
    "    for class_id, docs in enumerate(C, start=1):\n",
    "        for doc_id in docs:\n",
    "            doc_class_list[doc_id] = class_id\n",
    "\n",
    "    return doc_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43db623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_term_doc_matrix(V, C, path):\n",
    "    # V: 詞彙列表\n",
    "    # path: 包含文檔的目錄路徑\n",
    "\n",
    "    # 初始化詞彙-文檔矩陣\n",
    "    term_doc_matrix = np.zeros((len(V), len(C)))\n",
    "\n",
    "    # 建立詞彙索引字典\n",
    "    term_index = {term: idx for idx, term in enumerate(V)}\n",
    "    \n",
    "    dir_path = path\n",
    "    listdir = os.listdir(dir_path)\n",
    "\n",
    "    for i in range(1, len(C)+1):\n",
    "        data_path = os.path.join(dir_path, str(i) + '.txt')\n",
    "        term = preprocessing(data_path)\n",
    "\n",
    "        # 更新詞彙-文檔矩陣\n",
    "        for t in term:\n",
    "            if t in term_index:\n",
    "                term_doc_matrix[term_index[t], i-1] += 1\n",
    "\n",
    "    return term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09165a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_single_chisquare(chi_matrix):\n",
    "    class_num = 13\n",
    "    doc_num =15\n",
    "    i = 1\n",
    "    chi_score = 0\n",
    "    N_total = class_num * doc_num\n",
    "    for i in range(13):\n",
    "        small_chi_matrix = pd.DataFrame(columns=['present', 'absent'], index=[1,2])\n",
    "        small_chi_matrix = small_chi_matrix.replace(np.nan,0)\n",
    "        small_chi_matrix.iloc[0] = chi_matrix.iloc[i]\n",
    "        removed_chi_matrix = chi_matrix.drop([i+1])\n",
    "        small_chi_matrix.iat[1,0] = removed_chi_matrix['present'].sum()\n",
    "        small_chi_matrix.iat[1,1] = removed_chi_matrix['absent'].sum()\n",
    "        E11 =(small_chi_matrix.iat[0,0] + small_chi_matrix.iat[1,0]) * (small_chi_matrix.iat[0,0] + small_chi_matrix.iat[0,1])/N_total\n",
    "        E01 =(small_chi_matrix.iat[0,1] + small_chi_matrix.iat[1,1]) * (small_chi_matrix.iat[0,0] + small_chi_matrix.iat[0,1])/N_total\n",
    "        E10 =(small_chi_matrix.iat[0,0] + small_chi_matrix.iat[1,0]) * (small_chi_matrix.iat[1,0] + small_chi_matrix.iat[1,1])/N_total\n",
    "        E00 =(small_chi_matrix.iat[0,1] + small_chi_matrix.iat[1,1]) * (small_chi_matrix.iat[1,0] + small_chi_matrix.iat[1,1])/N_total\n",
    "        chi1 = (small_chi_matrix.iat[0,0] - E11)*(small_chi_matrix.iat[0,0] - E11)/E11\n",
    "        chi2 = (small_chi_matrix.iat[0,1] - E01)*(small_chi_matrix.iat[0,1] - E01)/E01\n",
    "        chi3 = (small_chi_matrix.iat[1,0] - E10)*(small_chi_matrix.iat[1,0] - E10)/E10\n",
    "        chi4 = (small_chi_matrix.iat[1,1] - E00)*(small_chi_matrix.iat[1,1] - E00)/E00\n",
    "        single_chi = chi1 + chi2 + chi3 + chi4\n",
    "        chi_score = chi_score + single_chi\n",
    "    chi_score = chi_score / class_num\n",
    "    return chi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccb70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_square_test(V, C, path):\n",
    "    total_score = list()\n",
    "    chi_score = list()\n",
    "    train_para_list = list()\n",
    "    for i in range(13):\n",
    "        single_para_list = list()\n",
    "        for j in range(15):\n",
    "            file_code = class_docid[i,j]\n",
    "            data_path = path + file_code + \".txt\"\n",
    "            term = preprocessing(data_path) \n",
    "            present_term = get_unique(term)\n",
    "            single_para_list.append(present_term)\n",
    "        train_para_list.append(single_para_list)\n",
    "\n",
    "    for k in range(len(V)):\n",
    "        chi_matrix = pd.DataFrame(columns=['present', 'absent'], index=[list(range(1,14,1))])\n",
    "        chi_matrix = chi_matrix.replace(np.nan,0)\n",
    "        for i in range(13):\n",
    "            for j in range(0,15):\n",
    "                if (V[k] in train_para_list[i][j] ):\n",
    "                    chi_matrix.iat[i,0] = chi_matrix.iat[i,0] + 1\n",
    "            chi_matrix.iat[i,1] = 15 - chi_matrix.iat[i,0]\n",
    "        total_score.append(cal_single_chisquare(chi_matrix))\n",
    "\n",
    "    current_score = total_score\n",
    "    current_score_1 = total_score\n",
    "    chisquare_before_select = pd.DataFrame()\n",
    "    chisquare_before_select = pd.concat([chisquare_before_select,pd.DataFrame(V)],axis = 1)\n",
    "    chisquare_before_select = pd.concat([chisquare_before_select,pd.DataFrame(current_score)],axis = 1)\n",
    "    chisquare_before_select.columns = [\"term\",\"chisquare\"]\n",
    "    chisquare_before_select.sort_values(\"chisquare\",inplace = True, ascending=False)\n",
    "    important_feature = chisquare_before_select[:500]\n",
    "    important_feature = list(important_feature[\"term\"])\n",
    "    V = np.array(important_feature)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eab62483",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = class_docid\n",
    "path = './data/'\n",
    "listdir = os.listdir(path)\n",
    "result = []\n",
    "#training\n",
    "V , V_ , prior, condprob = train_NB(C, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d007ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron' 'abandon' 'abc' ... 'zone' 'zoran' 'zutshi']\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd3f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15fd4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alberto' 'peru' 'fujimori' 'carnahan' 'ivori' 'australian' 'kursk'\n",
      " 'earthquak' 'vietnam' 'missouri' 'submarin' 'salvador' 'milosev'\n",
      " 'slobodan' 'convict' 'mel' 'escap' 'governor' 'hanoi' 'guei' 'cole'\n",
      " 'diver' 'torpedo' 'opposit' 'pardon' 'quak' 'pope' 'resign' 'scandal'\n",
      " 'vietnames' 'peruvian' 'sampra' 'match' 'tournament' 'barent' 'sport'\n",
      " 'nuclear' 'india' 'grand' 'alassan' 'ivorian' 'african' 'coup' 'ouattara'\n",
      " 'hawkin' 'belgrad' 'susan' 'kostunica' 'trial' 'elect' 'yugoslavia'\n",
      " 'texa' 'russian' 'destroy' 'coast' 'loui' 'asylum' 'uss' 'champion'\n",
      " 'japan' 'slam' 'peltier' 'guatemala' 'vojislav' 'safin' 'agassi'\n",
      " 'pakistan' 'marat' 'andr' 'abidjan' 'refuel' 'junta' 'yemen' 'pete'\n",
      " 'port' 'terrorist' 'eve' 'store' 'moscow' 'senat' 'clinton' 'crash'\n",
      " 'plane' 'christma' 'magnitud' 'robberi' 'central' 'terror' 'titl'\n",
      " 'cancer' 'player' 'vladimiro' 'draman' 'gujarat' 'edmund' 'aden' 'chri'\n",
      " 'jefferson' 'sifford' 'ashcroft' 'clemenc' 'salvadoran' 'seed' 'san'\n",
      " 'ship' 'secret' 'campaign' 'attack' 'seven' 'gen' 'communist' 'china'\n",
      " 'middl' 'landslid' 'prison' 'republican' 'asia' 'presid' 'mexico'\n",
      " 'relationship' 'explos' 'buri' 'panama' 'yugoslav' 'collaps' 'saigon'\n",
      " 'gbagbo' 'ricardo' 'coffe' 'peninsula' 'valentin' 'marquez' 'federico'\n",
      " 'sala' 'cocoa' 'laurent' 'tokyo' 'irv' 'escape' 'melbourn' 'leonard'\n",
      " 'whitewat' 'influenc' 'la' 'colina' 'norwegian' 'sea' 'visit' 'robert'\n",
      " 'lawyer' 'bone' 'court' 'agent' 'rule' 'leader' 'businessman' 'navi'\n",
      " 'pentagon' 'serbia' 'east' 'roger' 'larri' 'build' 'strike' 'sub'\n",
      " 'suprem' 'sailor' 'recov' 'brunei' 'vice' 'export' 'chi' 'bribe' 'minh'\n",
      " 'arabian' 'exclud' 'madrid' 'santa' 'conced' 'sentenc' 'respons' 'fbi'\n",
      " 'unifi' 'radioprograma' 'transpar' 'lima' 'montesino' 'paniagua' 'suicid'\n",
      " 'ferrero' 'ammunit' 'dock' 'milken' 'despic' 'cessna' 'miguel'\n",
      " 'davenport' 'worship' 'sunken' 'sank' 'lindsay' 'semifin' 'jack'\n",
      " 'arkansa' 'robber' 'cowardli' 'connal' 'trip' 'democrat' 'church' 'dirt'\n",
      " 'candid' 'play' 'injur' 'rescuer' 'cross' 'alert' 'decemb' 'toll' 'rescu'\n",
      " 'crimin' 'indian' 'summit' 'dig' 'isra' 'rubbl' 'congress' 'fugit' 'race'\n",
      " 'north' 'corrupt' 'america' 'western' 'son' 'boat' 'game' 'reserv'\n",
      " 'mission' 'dakota' 'good' 'militari' 'presidenti' 'spi' 'area' 'offic'\n",
      " 'intellig' 'dead' 'sway' 'polic' 'night' 'die' 'april' 'rice' 'twin'\n",
      " 'couldn' 'juan' 'epicent' 'activist' 'pine' 'ron' 'ridg' 'hull' 'white'\n",
      " 'war' 'buy' 'dalla' 'act' 'repatri' 'commerc' 'tudela' 'paddi' 'korean'\n",
      " 'traffick' 'lieuten' 'pow' 'hilli' 'leak' 'kenedi' 'deter' 'oshman'\n",
      " 'candidaci' 'aubrey' 'edmond' 'bedi' 'exclus' 'konan' 'uniform'\n",
      " 'palestinian' 'utterli' 'frantic' 'wimbledon' 'sele' 'sosonati' 'hondura'\n",
      " 'cellular' 'ana' 'monica' 'kuerten' 'parol' 'magana' 'martina' 'arturo'\n",
      " 'nepal' 'calvario' 'gustavo' 'tenni' 'junk' 'bond' 'sixth' 'venu' 'hingi'\n",
      " 'mass' 'delhi' 'wander' 'collis' 'coal' 'serb' 'serbian' 'mediat' 'vote'\n",
      " 'monday' 'trade' 'gov' 'bomb' 'kill' 'parent' 'civilian' 'electr'\n",
      " 'neighborhood' 'michael' 'suit' 'congressman' 'pacif' 'pilot' 'miss'\n",
      " 'bodi' 'former' 'weapon' 'aid' 'agreement' 'espionag' 'rev' 'knock'\n",
      " 'open' 'dive' 'polit' 'gore' 'israel' 'announc' 'tuesday' 'economi'\n",
      " 'accus' 'longtim' 'henri' 'death' 'least' 'lawmak' 'ceremoni' 'absolut'\n",
      " 'obstacl' 'speed' 'concret' 'antonio' 'control' 'suspect' 'men' 'process'\n",
      " 'damag' 'beat' 'bare' 'account' 'weather' 'hous' 'wall' 'econom' 'second'\n",
      " 'nation' 'won' 'depart' 'aftershock' 'west' 'identifi' 'defens' 'arena'\n",
      " 'kosovo' 'panic' 'woman' 'wilson' 'wreckag' 'weren' 'nativ' 'counti'\n",
      " 'commut' 'casualti' 'africa' 'russia' 'northwest' 'august' 'enforc'\n",
      " 'draw' 'request' 'chief' 'mile' 'constitut' 'ballot' 'amid' 'fraud'\n",
      " 'south' 'survivor' 'carlo' 'violenc' 'sound' 'debat' 'victim' 'american'\n",
      " 'run' 'ace' 'kafelnikov' 'aubri' 'maximum' 'serena' 'feloni'\n",
      " 'temporarili' 'pistol' 'girl' 'denver' 'bother' 'remiss' 'spotti' 'tore'\n",
      " 'sprain' 'harper' 'suchitepequez' 'ruin' 'notifi' 'marco' 'espi' 'tull'\n",
      " 'herd' 'yevgeni' 'seamen' 'shotgun' 'elbow' 'alongsid' 'honduran' 'riva'\n",
      " 'jaim' 'jalpataua' 'flore' 'retriev' 'freeh' 'holdup' 'vodafon'\n",
      " 'testimoni' 'disobedi' 'colo' 'rifl' 'compart' 'portugues' 'radar'\n",
      " 'ibero' 'mori' 'jung' 'farewel' 'dae' 'chelsea' 'encount' 'jiang'\n",
      " 'ancestr' 'societi' 'rolla' 'authoritarian' 'jerri' 'mourner' 'zemin'\n",
      " 'yoshiro' 'fist' 'launder' 'john' 'relat' 'francisco' 'declar' 'class'\n",
      " 'tom' 'health' 'employe' 'citi' 'govern' 'small' 'shovel' 'jean' 'runoff'\n",
      " 'shake' 'structur' 'danger' 'civil' 'aboard' 'parti' 'republ' 'sky']\n"
     ]
    }
   ],
   "source": [
    "print(V_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32eebbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "for i in range(1, len(listdir)+1):\n",
    "    if str(i) not in train_docid:\n",
    "        data_path = path + str(i) + \".txt\"\n",
    "        class_id = test_NB(C, V_, prior, condprob, i)\n",
    "        result.append([i, class_id])\n",
    "#save as result.csv\n",
    "df = pd.DataFrame(data = result, columns = [\"Id\", \"Value\"])\n",
    "df.to_csv(\"result.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c42baffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron' 'abandon' 'abc' ... 'zone' 'zoran' 'zutshi']\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d3ee6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11' '19' '29' '113' '115' '169' '278' '301' '316' '317' '321' '324'\n",
      "  '325' '338' '341']\n",
      " ['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '12' '13' '14' '15' '16']\n",
      " ['813' '817' '818' '819' '820' '821' '822' '824' '825' '826' '828' '829'\n",
      "  '830' '832' '833']\n",
      " ['635' '680' '683' '702' '704' '705' '706' '708' '709' '719' '720' '722'\n",
      "  '723' '724' '726']\n",
      " ['646' '751' '781' '794' '798' '799' '801' '812' '815' '823' '831' '839'\n",
      "  '840' '841' '842']\n",
      " ['995' '998' '999' '1003' '1005' '1006' '1007' '1009' '1011' '1012'\n",
      "  '1013' '1014' '1015' '1016' '1019']\n",
      " ['700' '730' '731' '732' '733' '735' '740' '744' '752' '754' '755' '756'\n",
      "  '757' '759' '760']\n",
      " ['262' '296' '304' '308' '337' '397' '401' '443' '445' '450' '466' '480'\n",
      "  '513' '533' '534']\n",
      " ['130' '131' '132' '133' '134' '135' '136' '137' '138' '139' '140' '141'\n",
      "  '142' '143' '145']\n",
      " ['31' '44' '70' '83' '86' '92' '100' '102' '305' '309' '315' '320' '326'\n",
      "  '327' '328']\n",
      " ['240' '241' '243' '244' '245' '248' '250' '254' '255' '256' '258' '260'\n",
      "  '275' '279' '295']\n",
      " ['535' '542' '571' '573' '574' '575' '576' '578' '581' '582' '583' '584'\n",
      "  '585' '586' '588']\n",
      " ['485' '520' '523' '526' '527' '529' '530' '531' '532' '536' '537' '538'\n",
      "  '539' '540' '541']]\n"
     ]
    }
   ],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f6054f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cla = generate_doc_class_list(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
